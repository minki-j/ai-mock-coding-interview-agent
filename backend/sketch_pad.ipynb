{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Docker container\n",
    "run this command to access its terminal\n",
    "```bash\n",
    "docker exec -it ai-mock-coding-interview-agent-fastapi-1 /bin/bash\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.main_graph import main_graph\n",
    "id = \"6734f462996d5ea7277bd903\"\n",
    "config = {\"configurable\": {\"thread_id\": id}, \"recursion_limit\": 100}\n",
    "state = main_graph.get_state(config).values\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_to_single_line(code_string: str) -> str:\n",
    "    # Replace tabs/spaces with \\t and newlines with \\n\n",
    "    lines = code_string.split(\"\\n\")\n",
    "    # Process each line to convert leading spaces/tabs to \\t\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        # Count leading spaces (assuming 4 spaces = 1 tab)\n",
    "        leading_spaces = len(line) - len(line.lstrip())\n",
    "        tab_count = leading_spaces // 4\n",
    "        # Replace the leading spaces with \\t characters\n",
    "        processed_line = \"\\t\" * tab_count + line.lstrip()\n",
    "        processed_lines.append(processed_line)\n",
    "\n",
    "    # Join with literal \\n\n",
    "    return \"\\\\n\".join(processed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from markdownify import markdownify\n",
    "\n",
    "path = \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/leetcode/scraped_data/longest-common-prefix.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "solution_md = markdownify(data[\"solution\"][\"content\"])\n",
    "content_md = markdownify(data[\"content\"])\n",
    "\n",
    "data[\"solution_md\"] = solution_md\n",
    "data[\"content_md\"] = content_md\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function that uses the first element of the list as a reference.\n",
    "2. Iteratively check if the first n characters of the reference are included in all other elements.\n",
    "3. Increase n until finding the longest common prefix.\n",
    "4. Add an assertion to check for empty elements in the array.\n",
    "5. Consider handling special characters as an edge case.\n",
    "6. Implement the solution and analyze time complexity during coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interview_question': str,\n",
       " 'interview_solution': str,\n",
       " 'code_editor_state': str,\n",
       " 'debugging_record': list[str]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class DebuggingAgentPrivateState(BaseModel):\n",
    "    interview_question: str = Field(default=\"\")\n",
    "    interview_solution: str = Field(default=\"\")\n",
    "    code_editor_state: str = Field(default=\"\")\n",
    "\n",
    "    debugging_record: list[str] = Field(default=[])\n",
    "\n",
    "state = DebuggingAgentPrivateState()\n",
    "state.__annotations__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# populate prep_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "with open(\n",
    "    \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/leetcode.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    allquestions = json.load(f)\n",
    "\n",
    "id_title_dict = {}\n",
    "for question in allquestions:\n",
    "    id_title_dict[question[\"title\"].replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").lower()] = question[\"id\"]\n",
    "\n",
    "print(id_title_dict[\"twosum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import random\\nimport string\\nimport unittest\\n\\nfrom hypothesis import given\\nfrom hypothesis.strategies import text\\n\\n\\nclass Test(unittest.TestCase):\\n    def test_1(self):\\n        solution = Solution()\\n        self.assertEqual(solution.findTheDifference(\"abcd\", \"abcde\"), \"e\")\\n\\n    @given(text())\\n    def test_random(self, s):\\n        solution = Solution()\\n        random_letter = random.choice(string.ascii_letters)\\n        t = list(s + random_letter)\\n        random.shuffle(t)\\n        t = \"\".join(t)\\n        self.assertEqual(solution.findTheDifference(s, t), random_letter)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "prep_code_dict = {}\n",
    "test_code_dict = {}\n",
    "\n",
    "path = \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/prep\"\n",
    "for id in os.listdir(path):\n",
    "    if \"test\" in id:\n",
    "        file_path = os.path.join(path, id)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            test_code_dict[id.split(\"_\")[0]] = f.read()\n",
    "\n",
    "    file_path = os.path.join(path, id)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        prep_code_dict[id.split(\".\")[0]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> 3sum\n",
      "==>> longestpalindromicsubstring\n",
      "==>> validparentheses\n",
      "==>> nextpermutation\n",
      "==>> binarytreeinordertraversal\n",
      "==>> reverseinteger\n",
      "==>> containerwithmostwater\n",
      "==>> mergeintervals\n",
      "==>> addtwonumbers\n",
      "==>> longestsubstringwithoutrepeatingcharacters\n",
      "==>> palindromenumber\n",
      "==>> longestcommonprefix\n",
      "==>> mergesortedarray\n",
      "==>> 4sum\n",
      "==>> twosum\n",
      "==>> medianoftwosortedarrays\n",
      "==>> insertinterval\n",
      "==>> swapnodesinpairs\n",
      "==>> lengthoflastword\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/leetcode/refined_data\"\n",
    "new_path = \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/leetcode/complete_data\"\n",
    "\n",
    "for question in os.listdir(path):\n",
    "    title = question.split(\".\")[0].replace(\" \", \"\").replace(\"-\", \"\").replace(\"_\", \"\").lower()\n",
    "    id = id_title_dict.get(title)\n",
    "\n",
    "    prep_code = prep_code_dict.get(id)\n",
    "    if prep_code:\n",
    "        print(f\"==>> {title}\")\n",
    "        with open(os.path.join(path, question), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        data[\"prep_code\"] = prep_code\n",
    "        data[\"test_code\"] = test_code_dict.get(id)\n",
    "\n",
    "        if data.get(\"solution_md\"):\n",
    "            with open(os.path.join(new_path, question), \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "        else:\n",
    "            with open(os.path.join(path, question), \"w\") as f:\n",
    "                json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "new_path = \"/Users/minkijung/Documents/2PetProjects/ai-mock-coding-interview-agent/backend/db/leetcode/complete_data\"\n",
    "\n",
    "for question in os.listdir(new_path):\n",
    "    with open(os.path.join(new_path, question), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    data[\"content_md\"] = markdownify(data[\"content\"]).replace(\"\\n\\n\\u00a0\\n\\n\", \"\")\n",
    "    solution = data[\"solution\"][\"content\"]\n",
    "    solution = (\n",
    "        solution.replace('<iframe src=\"\\n', \"\")\n",
    "        .replace('\"></iframe>', \"\")\n",
    "        .replace(\"\", \"\")\n",
    "    )\n",
    "    solution = re.sub(r\"frameBorder.*\\n\", \"\", solution)\n",
    "    solution = solution.replace('```\\n\"', \"```\\n\")\n",
    "\n",
    "    data[\"solution_md\"] = markdownify(solution)\n",
    "\n",
    "    approachs = solution.split(\"\\n---\\n\\n### Approach \")[1:]\n",
    "    data[\"approaches\"] = []\n",
    "    for i, approach in enumerate(approachs):\n",
    "        data[\"approaches\"].append(\n",
    "            {\n",
    "                \"title\": approach.split(\"\\n\")[0].replace(f\"{i+1}: \", \"\").strip(),\n",
    "                \"approach\": \"\\n\".join(approach.split(\"\\n\")[1:]).strip(),\n",
    "            }\n",
    "        )\n",
    "    with open(os.path.join(new_path, question), \"w\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node1_func(state):\n",
    "    print(\"==>> node1_func\")\n",
    "    return {\"middle_way\": \"Hello \" + state.input_message}\n",
    "\n",
    "def node2_func(state):\n",
    "    print(\"==>> node2_func\")\n",
    "    return {\"middle_way\": \"emptied middle way at node2\"}\n",
    "\n",
    "def node1_func_sub2(state):\n",
    "    print(\"==>> node1_func_sub2\")\n",
    "    return {\"middle_way\": \"sub2 changed middle way\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "class OverallState(BaseModel):\n",
    "  input_message: str = Field(default=\"\")\n",
    "  middle_way: str = Field(default=\"\")\n",
    "\n",
    "g_sub2 = StateGraph(OverallState)\n",
    "g_sub2.add_edge(START, \"node1\")\n",
    "g_sub2.add_node(\"node1\", node1_func_sub2)\n",
    "g_sub2.add_edge(\"node1\", END)\n",
    "\n",
    "graph_sub2 = g_sub2.compile(checkpointer=MemorySaver(), interrupt_after=[\"node1\"])\n",
    "\n",
    "g_sub = StateGraph(OverallState)\n",
    "g_sub.add_edge(START, \"node1\")\n",
    "g_sub.add_node(\"node1\", node1_func)\n",
    "g_sub.add_edge(\"node1\", \"node2\")\n",
    "g_sub.add_node(\"node2\", node2_func)\n",
    "g_sub.add_edge(\"node2\", \"graph_sub2\")\n",
    "g_sub.add_node(\"graph_sub2\", graph_sub2)\n",
    "g_sub.add_edge(\"graph_sub2\", END)\n",
    "\n",
    "graph_sub = g_sub.compile(checkpointer=MemorySaver(), interrupt_after=[\"node1\"])\n",
    "\n",
    "g = StateGraph(OverallState)\n",
    "g.add_edge(START, \"graph_sub\")\n",
    "g.add_node(\"graph_sub\", graph_sub)\n",
    "g.add_edge(\"graph_sub\", END)\n",
    "\n",
    "graph = g.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> node1_func\n",
      "output_middle_way:  .\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": 2}}\n",
    "\n",
    "output_middle_way = graph.invoke({\"input_message\": \"Minki\", \"middle_way\": \"\"}, config)\n",
    "print(\"output_middle_way:\", output_middle_way[\"middle_way\"], \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: ()\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state(config, subgraphs=True)\n",
    "print(\"state:\", state.tasks[0].state.tasks[0].state.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> node2_func\n",
      "==>> node1_func_sub2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_message': 'Minki', 'middle_way': ''}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(None, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(config, subgraphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PregelTask(id='9b102e20-0b2c-98d4-c209-bad4d14d2940', name='graph_sub2', path=('__pregel_pull', 'graph_sub2'), error=None, interrupts=(), state=StateSnapshot(values={'input_message': 'Minki', 'middle_way': 'sub2 changed middle way'}, next=(), config={'configurable': {'thread_id': 2, 'checkpoint_ns': 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074|graph_sub2:9b102e20-0b2c-98d4-c209-bad4d14d2940', 'checkpoint_id': '1efaffef-f23c-6d2e-8001-c7b1315e36e9', 'checkpoint_map': {'': '1efaffec-4c24-691c-8000-f49bce64ead9', 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074': '1efaffef-f236-6988-8002-9c3f33d2664e', 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074|graph_sub2:9b102e20-0b2c-98d4-c209-bad4d14d2940': '1efaffef-f23c-6d2e-8001-c7b1315e36e9'}}}, metadata={'source': 'loop', 'writes': {'node1': {'middle_way': 'sub2 changed middle way'}}, 'thread_id': 2, 'langgraph_step': 3, 'langgraph_node': 'graph_sub2', 'langgraph_triggers': ['node2'], 'langgraph_path': ['__pregel_pull', 'graph_sub2'], 'langgraph_checkpoint_ns': 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074|graph_sub2:9b102e20-0b2c-98d4-c209-bad4d14d2940', 'checkpoint_ns': 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074', 'step': 1, 'parents': {'': '1efaffec-4c24-691c-8000-f49bce64ead9', 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074': '1efaffef-f236-6988-8002-9c3f33d2664e'}}, created_at='2024-12-01T16:12:00.886492+00:00', parent_config={'configurable': {'thread_id': 2, 'checkpoint_ns': 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074|graph_sub2:9b102e20-0b2c-98d4-c209-bad4d14d2940', 'checkpoint_id': '1efaffef-f23b-646a-8000-070b3dccaa1d', 'checkpoint_map': {'': '1efaffec-4c24-691c-8000-f49bce64ead9', 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074': '1efaffef-f236-6988-8002-9c3f33d2664e', 'graph_sub:f866c5e5-f65f-8347-3c36-021e2ac97074|graph_sub2:9b102e20-0b2c-98d4-c209-bad4d14d2940': '1efaffef-f23b-646a-8000-070b3dccaa1d'}}}, tasks=()), result=None),)\n"
     ]
    }
   ],
   "source": [
    "print(state.tasks[0].state.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: {'input_message': 'Minki', 'middle_way': 'Hello Minki'}\n",
      "subgraph_config: {'configurable': {'thread_id': 2, 'checkpoint_ns': 'graph_sub:08b1cd42-d26a-628d-dd6b-30bdaf6f4ec2', 'checkpoint_id': '1efafeaa-aba1-63c4-8001-4a5e5903d01e', 'checkpoint_map': {'': '1efafeaa-ab9a-6024-8000-5cc1cb67c673', 'graph_sub:08b1cd42-d26a-628d-dd6b-30bdaf6f4ec2': '1efafeaa-aba1-63c4-8001-4a5e5903d01e'}}}\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state(config, subgraphs=True)\n",
    "print(\"state:\", state.tasks[0].state.values)\n",
    "subgraph_config = state.tasks[0].state.config\n",
    "print(\"subgraph_config:\", subgraph_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> node2_func\n",
      "output_final: emptied middle way at node2 .\n"
     ]
    }
   ],
   "source": [
    "output_final = graph.invoke(None, config)\n",
    "print(\"output_final:\", output_final[\"middle_way\"], \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: ()\n",
      "state: <class 'tuple'>\n",
      "no tasks\n"
     ]
    }
   ],
   "source": [
    "state = graph.get_state(config, subgraphs=True)\n",
    "print(\"state:\", state.tasks)\n",
    "print(\"state:\", type(state.tasks))\n",
    "if not state.tasks:\n",
    "    print(\"no tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution(object):\n",
    "    # method to check if a part of the string is within the range 0-255,\n",
    "    # returns True if part is within range 0-255 else False\n",
    "    def valid(self, s, start, length):\n",
    "        return length == 1 or (\n",
    "            s[start] != \"0\" and (length < 3 or s[start : start + length] <= \"255\")\n",
    "        )\n",
    "\n",
    "    # main helper method to solve the problem by backtracking\n",
    "    def helper(self, s, startIndex, dots, ans):\n",
    "        remainingLength = len(s) - startIndex\n",
    "        remainingNumberOfIntegers = 4 - len(dots)\n",
    "        if (\n",
    "            remainingLength > remainingNumberOfIntegers * 3\n",
    "            or remainingLength < remainingNumberOfIntegers\n",
    "        ):\n",
    "            return\n",
    "        if len(dots) == 3:\n",
    "            if self.valid(s, startIndex, remainingLength):\n",
    "                temp = \"\"\n",
    "                last = 0\n",
    "                for dot in dots:\n",
    "                    temp += s[last : last + dot] + \".\"\n",
    "                    last += dot\n",
    "                temp += s[startIndex:]\n",
    "                ans.append(temp)\n",
    "            return\n",
    "        for curPos in range(1, min(4, remainingLength + 1)):\n",
    "            dots.append(curPos)\n",
    "            if self.valid(s, startIndex, curPos):\n",
    "                self.helper(s, startIndex + curPos, dots, ans)\n",
    "            dots.pop()\n",
    "\n",
    "    # main method called by leetcode\n",
    "    def restoreIpAddresses(self, s):\n",
    "        answer = []\n",
    "        self.helper(s, 0, [], answer)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['255.255.11.135', '255.255.111.35']\n"
     ]
    }
   ],
   "source": [
    "solution = Solution()\n",
    "result = solution.restoreIpAddresses(\"12.345.678.901.234.567.890\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
